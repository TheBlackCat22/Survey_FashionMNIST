{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Flattened Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = FashionMNIST(root='./Data', download=True, train=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(mean=0.2860, std=0.3530)]))\n",
    "test_data = FashionMNIST(root='./Data', download=True, train=False, transform=transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(mean=0.2860, std=0.3530)]))\n",
    "\n",
    "train_data, val_data = random_split(\n",
    "    train_data, [55000, 5000], generator=torch.Generator().manual_seed(10))\n",
    "\n",
    "data = {'train': train_data,\n",
    "        'val': val_data,\n",
    "        'test': test_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting train Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 430/430 [00:09<00:00, 44.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting val Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 64.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting test Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:00<00:00, 82.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Dict Saved to,  ./Data/Features/flattened_features.pt\n"
     ]
    }
   ],
   "source": [
    "# feature size should be nx(28*28)\n",
    "\n",
    "save_path = './Data/Features/flattened_features.pt'\n",
    "batch_size = 128\n",
    "\n",
    "features = {}\n",
    "for key, value in data.items():\n",
    "    print(f'\\nExtracting {key} Features...')\n",
    "\n",
    "    loader = DataLoader(value, batch_size, shuffle=False, num_workers=3)\n",
    "\n",
    "    running_features, running_labels = torch.tensor([]), torch.tensor([])\n",
    "    for images, labels in tqdm(loader):\n",
    "        running_features = torch.cat(\n",
    "            [running_features, torch.flatten(images, start_dim=1)], dim=0)\n",
    "        running_labels = torch.cat([running_labels, labels], dim=0)\n",
    "\n",
    "    features[key] = [running_features, running_labels]\n",
    "\n",
    "torch.save(features, save_path)\n",
    "print(\"\\nFeature Dict Saved to, \", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Resnet18 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "train_data = FashionMNIST(root='./Data', download=True, train=True, transform=transforms.Compose(\n",
    "    [transforms.Lambda(lambda x: x.convert('RGB')), ResNet18_Weights.IMAGENET1K_V1.transforms(antialias=True)]))\n",
    "test_data = FashionMNIST(root='./Data', download=True, train=False, transform=transforms.Compose(\n",
    "    [transforms.Lambda(lambda x: x.convert('RGB')), ResNet18_Weights.IMAGENET1K_V1.transforms(antialias=True)]))\n",
    "\n",
    "train_data, val_data = random_split(\n",
    "    train_data, [55000, 5000], generator=torch.Generator().manual_seed(10))\n",
    "\n",
    "data = {'train': train_data,\n",
    "        'val': val_data,\n",
    "        'test': test_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = resnet18()\n",
    "# Weights from https://download.pytorch.org/models/resnet18-f37072fd.pth\n",
    "model.load_state_dict(torch.load('./Data/Models/resnet18-f37072fd.pth'))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting train Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 430/430 [01:35<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting val Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:08<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting test Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:16<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Dict Saved to,  ./Data/Features/resnet18_features.pt\n"
     ]
    }
   ],
   "source": [
    "# feature size should be nx1000\n",
    "\n",
    "save_path = './Data/Features/resnet18_features.pt'\n",
    "batch_size = 128\n",
    "num_workers = 5\n",
    "\n",
    "features = {}\n",
    "for key, value in data.items():\n",
    "    print(f'\\nExtracting {key} Features...')\n",
    "\n",
    "    loader = DataLoader(value, batch_size, num_workers=num_workers)\n",
    "\n",
    "    running_features, running_labels = torch.tensor([]), torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader):\n",
    "            images = images.to(device)\n",
    "            running_features = torch.cat(\n",
    "                [running_features, model(images).to('cpu')], dim=0)\n",
    "            running_labels = torch.cat([running_labels, labels], dim=0)\n",
    "\n",
    "    features[key] = [running_features, running_labels]\n",
    "\n",
    "torch.save(features, save_path)\n",
    "print(\"\\nFeature Dict Saved to, \", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Vit Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import ViT_B_16_Weights\n",
    "\n",
    "train_data = FashionMNIST(root='./Data', download=True, train=True, transform=transforms.Compose(\n",
    "    [transforms.Lambda(lambda x: x.convert('RGB')), ViT_B_16_Weights.IMAGENET1K_V1.transforms(antialias=True)]))\n",
    "test_data = FashionMNIST(root='./Data', download=True, train=False, transform=transforms.Compose(\n",
    "    [transforms.Lambda(lambda x: x.convert('RGB')), ViT_B_16_Weights.IMAGENET1K_V1.transforms(antialias=True)]))\n",
    "\n",
    "train_data, val_data = random_split(\n",
    "    train_data, [55000, 5000], generator=torch.Generator().manual_seed(10))\n",
    "\n",
    "data = {'train': train_data,\n",
    "        'val': val_data,\n",
    "        'test': test_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vit_b_16\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = vit_b_16()\n",
    "# Weights from https://download.pytorch.org/models/vit_b_16-c867db91.pth\n",
    "model.load_state_dict(torch.load('./Data/Models/vit_b_16-c867db91.pth'))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting train Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 430/430 [23:58<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting val Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [02:09<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting test Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [04:19<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Dict Saved to,  ./Data/Features/vit_b_16_features.pt\n"
     ]
    }
   ],
   "source": [
    "# feature size should be nx1000\n",
    "\n",
    "save_path = './Data/Features/vit_b_16_features.pt'\n",
    "batch_size = 128\n",
    "num_workers = 5\n",
    "\n",
    "features = {}\n",
    "for key, value in data.items():\n",
    "    print(f'\\nExtracting {key} Features...')\n",
    "\n",
    "    loader = DataLoader(value, batch_size, num_workers=num_workers)\n",
    "\n",
    "    running_features, running_labels = torch.tensor([]), torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader):\n",
    "            images = images.to(device)\n",
    "            running_features = torch.cat(\n",
    "                [running_features, model(images).to('cpu')], dim=0)\n",
    "            running_labels = torch.cat([running_labels, labels], dim=0)\n",
    "\n",
    "    features[key] = [running_features, running_labels]\n",
    "\n",
    "torch.save(features, save_path)\n",
    "print(\"\\nFeature Dict Saved to, \", save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmnist_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
