{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGfSHioz4a-y"
      },
      "source": [
        "# Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lt6OqayS4a-0",
        "outputId": "45a9ddca-bbbe-4ddb-8bcf-4d69de57792e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data (X, y):  (60000, 768) (60000,)\n",
            "Test Data (X, y):  (10000, 768) (10000,)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "# 'vit_b_16_features.pt' file can be obtained by running 'feature_extraction.ipynb'\n",
        "feature_path = './Data/Features/vit_b_16_features.pt'\n",
        "data = torch.load(feature_path)\n",
        "for key, value in data.items():\n",
        "    print(f'{key.capitalize()} Data (X, y): ', value[0].shape, value[1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gIDR7pt4a-1"
      },
      "source": [
        "# Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DogHqZh84a-2",
        "outputId": "07b38981-a3b2-461c-e9c2-73188b01f6cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data (X, y):  (55000, 768) (55000,)\n",
            "Test Data (X, y):  (10000, 768) (10000,)\n",
            "Val Data (X, y):  (5000, 768) (5000,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    data['train'][0], data['train'][1], test_size=5000, stratify=data['train'][1], random_state=10)\n",
        "data['train'] = [X_train, y_train]\n",
        "data['val'] = [X_val, y_val]\n",
        "\n",
        "for key, value in data.items():\n",
        "    print(f'{key.capitalize()} Data (X, y): ', value[0].shape, value[1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lAZagce4a-2"
      },
      "source": [
        "# Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSGStzzq4a-2"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29smKh2y4a-2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "pca = PCA()\n",
        "scalar = StandardScaler().fit(data['train'][0])\n",
        "pca.fit(scalar.transform(data['train'][0]))\n",
        "explained_variance_ratios = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "data['train'][0] = pca.transform(scalar.transform(data['train'][0]))\n",
        "data['val'][0] = pca.transform(scalar.transform(data['val'][0]))\n",
        "data['test'][0] = pca.transform(scalar.transform(data['test'][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD2dQkYm4a-3"
      },
      "source": [
        "## Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5n-7HtY4a-3"
      },
      "outputs": [],
      "source": [
        "scalar = StandardScaler().fit(*data['train'])\n",
        "\n",
        "data['train'][0] = scalar.transform(data['train'][0])\n",
        "data['val'][0] = scalar.transform(data['val'][0])\n",
        "data['test'][0] = scalar.transform(data['test'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNiYXfXw4a-3"
      },
      "source": [
        "## Anova Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WJDvwSz4a-3"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "scores, _ = f_classif(*data['train'])\n",
        "\n",
        "feature_score = list(zip(range(0, data['train'][0].shape[1]), scores))\n",
        "feature_score = sorted(feature_score, key=lambda item: item[1], reverse=True)\n",
        "\n",
        "feature_ordering = [item[0] for item in feature_score]\n",
        "\n",
        "data['train'][0] = data['train'][0][:, feature_ordering]\n",
        "data['val'][0] = data['val'][0][:, feature_ordering]\n",
        "data['test'][0] = data['test'][0][:, feature_ordering]\n",
        "\n",
        "explained_variance_ratios = np.cumsum(\n",
        "    pca.explained_variance_ratio_[feature_ordering])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Leo8rbY_4a-4"
      },
      "source": [
        "## Feature Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qki1j79L4a-4"
      },
      "outputs": [],
      "source": [
        "n_component = 278"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDnV2XMQ4a-4"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fklA_mPA4a-4"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(10)\n",
        "\n",
        "val_f1_scores = []\n",
        "for C in tqdm([0.1, 1, 10, 25]):\n",
        "\n",
        "    for k in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
        "\n",
        "        if k == 'poly':\n",
        "          for degree in [2, 3 ,4]:\n",
        "              clf = SVC(C = C, kernel = k, degree = degree).fit(data['train'][0][:,:n_component], data['train'][1])\n",
        "              val_preds = clf.predict(data['val'][0][:,:n_component])\n",
        "              s = f1_score(data['val'][1], val_preds, average='macro')\n",
        "              val_f1_scores.append(s)\n",
        "              torch.save(clf, './Data/Models/HP_SVM_C'+str(C)+'_k_poly_d'+str(degree)+'_f1score_'+str(round(s,4))+'.pt')\n",
        "\n",
        "        else:\n",
        "              clf = SVC(C = C, kernel = k).fit(data['train'][0][:,:n_component], data['train'][1])\n",
        "              val_preds = clf.predict(data['val'][0][:,:n_component])\n",
        "              s = f1_score(data['val'][1], val_preds, average='macro')\n",
        "              val_f1_scores.append(s)\n",
        "              torch.save(clf, './Data/Models/HP_SVM_C'+str(C)+'_k_'+k+'_f1score_'+str(round(s,4))+'.pt')\n",
        "\n",
        "plt.plot(range(1, len(val_f1_scores)+1), val_f1_scores, label = 'Val')\n",
        "plt.xlabel('n_component')\n",
        "plt.ylabel('f1-Score')\n",
        "plt.suptitle('SVM')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print('Maximum Validation F1-Score: ', max(val_f1_scores))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}